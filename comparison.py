'''
comparison.py
This file compares the 3 models on a single image
'''
import torch
from torchvision.transforms import ToTensor, Normalize, Resize
from transformers import SegformerForSemanticSegmentation, AutoImageProcessor
from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import cv2
import torch.nn.functional as F

# ==== Constants ====
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
UNET_IMAGE_SIZE = (256, 256)  # UNet input size
SEGFORMER_SAM_IMAGE_SIZE = (512, 512)  # SegFormer and SAM input size
NUM_CLASSES = 29  # Update according to your dataset

# ==== Dataset Class ====
class KittiNPYDataset:
    def __init__(self, images_path, masks_path):
        self.images = np.load(images_path)
        self.masks = np.load(masks_path) if masks_path else None
        print(f"Loaded dataset: {len(self.images)} images.")
        if masks_path:
            print(f"Masks shape: {self.masks.shape}")

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        mask = self.masks[idx] if self.masks is not None else None
        return image, mask

# ==== Load Models ====
# Load UNet model
unet_model = tf.keras.models.load_model("Segmentation-Model-UNET.h5")

# Load SegFormer model
segformer_model = SegformerForSemanticSegmentation.from_pretrained(
    "nvidia/segformer-b0-finetuned-ade-512-512",
    num_labels=NUM_CLASSES,
    id2label={str(i): str(i) for i in range(NUM_CLASSES)},
    label2id={str(i): i for i in range(NUM_CLASSES)},
    ignore_mismatched_sizes=True
)
segformer_model.load_state_dict(torch.load(r"segformer_model.pth", map_location=DEVICE))
segformer_model.to(DEVICE)
segformer_processor = AutoImageProcessor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")

# Initialize SAM
def initialize_sam():
    print("Initializing SAM...")
    sam = sam_model_registry["vit_h"](checkpoint=r"C:\Users\yutik\Semantic-segmentation\sam_vit_h_4b8939.pth")
    sam.to(DEVICE)
    return SamAutomaticMaskGenerator(sam)

mask_generator = initialize_sam()

# ==== Helper Functions ====
def preprocess_image(image):
    """
    Preprocess the image for all models, considering different input sizes.
    Args:
        image (np.ndarray): Input image array.
    Returns:
        dict: Preprocessed inputs for UNet, SegFormer, and SAM.
    """
    # Ensure image is in range [0, 255] and has 3 channels
    if image.dtype != np.uint8:
        if image.max() <= 1.0:
            image = (image * 255).astype(np.uint8)
        else:
            image = image.astype(np.uint8)

    # Resize image for UNet (256x256)
    unet_image = np.array(Image.fromarray(image).resize(UNET_IMAGE_SIZE))

    # Preprocessing for UNet (TensorFlow)
    unet_input = unet_image / 255.0  # Normalize to [0, 1]
    unet_input = np.expand_dims(unet_input, axis=0)  # Add batch dimension

    # Resize image for SegFormer and SAM (512x512)
    segformer_sam_image = np.array(Image.fromarray(image).resize(SEGFORMER_SAM_IMAGE_SIZE))

    # Preprocessing for SegFormer (PyTorch)
    segformer_input = segformer_processor(images=segformer_sam_image, return_tensors="pt")
    segformer_input = {k: v.to(DEVICE) for k, v in segformer_input.items()}

    return {
        "unet": unet_input,
        "segformer": segformer_input,
        "sam": segformer_sam_image,
        "original": image  # Original for visualization
    }

def get_segformer_prediction(model, inputs):
    """
    Get predictions from SegFormer and upsample to match input size.
    """
    outputs = model(pixel_values=inputs["pixel_values"])
    preds = torch.argmax(outputs.logits, dim=1).squeeze(0)  # Shape: (128, 128)
    # Upsample to match original image size
    preds = F.interpolate(preds.unsqueeze(0).unsqueeze(0).float(), 
                          size=SEGFORMER_SAM_IMAGE_SIZE, 
                          mode="nearest").squeeze().cpu().numpy()  # Shape: (512, 512)
    return preds

def get_sam_prediction(mask_generator, image):
    """
    Generate a combined mask from SAM with each mask having unique values.
    """
    masks = mask_generator.generate(image)
    print(f"Generated {len(masks)} masks.")

    if len(masks) == 0:
        print("No masks generated by SAM.")
        return np.zeros(image.shape[:2], dtype=np.uint8)

    # Create an empty combined mask
    combined_mask = np.zeros(image.shape[:2], dtype=np.uint8)
    
    for idx, mask_data in enumerate(masks):
        # Assign unique values to each mask
        combined_mask[mask_data['segmentation']] = idx + 1  # Avoid 0 for background
    
    return combined_mask



def visualize_results(image, unet_mask, segformer_mask, sam_combined_mask, ground_truth=None):
    """
    Visualize input image and predictions from all models, including SAM combined mask.
    """
    print(f"Original Image Shape: {image.shape}")
    print(f"UNet Mask Shape: {unet_mask.shape}")
    print(f"SegFormer Mask Shape: {segformer_mask.shape}")
    print(f"SAM Combined Mask Shape: {sam_combined_mask.shape}")
    if ground_truth is not None:
        print(f"Ground Truth Shape: {ground_truth.shape}")

    plt.figure(figsize=(15, 10))

    # Input Image
    plt.subplot(2, 3, 1)
    plt.imshow(image.astype(np.uint8))
    plt.title("Input Image")
    plt.axis("off")

    # Ground Truth
    if ground_truth is not None:
        plt.subplot(2, 3, 2)
        plt.imshow(ground_truth, cmap="viridis")
        plt.title("Ground Truth")
        plt.axis("off")

    # UNet Prediction
    plt.subplot(2, 3, 3)
    plt.imshow(unet_mask, cmap="viridis")
    plt.title("UNet Prediction")
    plt.axis("off")

    # SegFormer Prediction
    plt.subplot(2, 3, 4)
    plt.imshow(segformer_mask, cmap="viridis")
    plt.title("SegFormer Prediction")
    plt.axis("off")

    # SAM Combined Prediction
    plt.subplot(2, 3, 5)
    plt.imshow(sam_combined_mask, cmap="viridis")
    plt.title("SAM Prediction")
    plt.axis("off")

    plt.tight_layout()
    plt.show()

# ==== Main Code ====
print("\nLoading dataset...")
val_dataset = KittiNPYDataset(
    images_path=r"C:\Users\yutik\Semantic-segmentation\kitti_test_images.npy",
    masks_path=r"C:\Users\yutik\Semantic-segmentation\kitti_test_masks.npy"
)

for idx in range(len(val_dataset)):
    image, ground_truth = val_dataset[idx]

    # Preprocess the image
    inputs = preprocess_image(image)

    # Reduce ground truth to 2D if it has multiple channels
    ground_truth = ground_truth.argmax(axis=-1) if ground_truth is not None else None

    # UNet Prediction
    unet_prediction = unet_model.predict(inputs["unet"])[0].argmax(axis=-1)  

    # SegFormer Prediction
    segformer_prediction = get_segformer_prediction(segformer_model, inputs["segformer"])  

    # SAM Prediction
    sam_prediction = get_sam_prediction(mask_generator, inputs["sam"]) 

    # Visualize Results
    visualize_results(inputs["original"], unet_prediction, segformer_prediction, sam_prediction, ground_truth)
